INFO:datasets:PyTorch version 2.5.1+cpu available.
Loading dataset...
Finished loading dataset.
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:02<00:35,  2.54s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:04<00:29,  2.29s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:06<00:25,  2.10s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:08<00:22,  2.01s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:11<00:22,  2.24s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:15<00:27,  3.07s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:18<00:23,  2.89s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:22<00:24,  3.44s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:26<00:20,  3.47s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:31<00:19,  3.87s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:35<00:15,  3.99s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:35<00:00,  2.36s/it]
WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.
INFO:Llama-70B-MAIN:Starting Benchmark run
/home/cmuser/venv/cm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/cmuser/venv/cm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/cmuser/venv/cm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(

No warnings encountered during test.

No errors encountered during test.
INFO:Llama-70B-MAIN:Run Completed!
INFO:Llama-70B-MAIN:Destroying SUT...
INFO:Llama-70B-MAIN:Destroying QSL...
Loaded model
Loaded tokenizer
IssueQuery started with 1 samples
IssueQuery done
Saving outputs to run_outputs/q0.pkl
Samples run: 1
	BatchMaker time: 0.08773922920227051
	Inference time: 3533.4912979602814
	Postprocess time: 0.11108136177062988
	==== Total time: 3533.6901185512543
